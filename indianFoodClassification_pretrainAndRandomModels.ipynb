{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nqu2YZLhlS8A"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S-YxQSMkoO_O"
      },
      "outputs": [],
      "source": [
        "# kaggle.json contains the necessary API username and key to use Kaggle\n",
        "!ls -lha kaggle.json &> /dev/null "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7nU8iS-obDW"
      },
      "outputs": [],
      "source": [
        "!mkdir -p ~/.kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wY0scvCLojty"
      },
      "outputs": [],
      "source": [
        "!cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WsZUBjt1onEY"
      },
      "outputs": [],
      "source": [
        "# hides Kaggle API key from users on server\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofZFEwCbqhz4"
      },
      "outputs": [],
      "source": [
        "!kaggle datasets download -d iamsouravbanerjee/indian-food-images-dataset &> /dev/null \n",
        "!kaggle datasets download -d anshulmehtakaggl/themassiveindianfooddataset &> /dev/null \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-q7xQyGp7Pno"
      },
      "outputs": [],
      "source": [
        "!unzip /content/indian-food-images-dataset.zip &> /dev/null \n",
        "!unzip /content/themassiveindianfooddataset.zip &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBq2M4Xc-Usg"
      },
      "outputs": [],
      "source": [
        "!mkdir our_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oQtocfJ4ANoi"
      },
      "outputs": [],
      "source": [
        "!rm -rf themassiveindianfooddataset.zip indian-food-images-dataset.zip sample_data/ food20dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hS7M_5OJdMd"
      },
      "outputs": [],
      "source": [
        "# move all files into the folder our_data\n",
        "with open(\"List of Indian Foods.txt\", \"r\") as a_file:\n",
        "  for line in a_file:\n",
        "    stripped_line = line.strip()\n",
        "    !mv \"/content/Indian Food Images/Indian Food Images/{stripped_line}\" \"/content/our_data\" &> /dev/null \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AKtW5IOR0EK8"
      },
      "outputs": [],
      "source": [
        "# most subfolders were not removed from the Indian Food Images folder, so we repeat the process\n",
        "with open(\"List of Indian Foods.txt\", \"r\") as a_file:\n",
        "  for line in a_file:\n",
        "    stripped_line = line.strip()\n",
        "    !mv \"/content/our_data/Indian Food Images/{stripped_line}\" \"/content/our_data\" &> /dev/null "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rL1FM_smJ7Is"
      },
      "outputs": [],
      "source": [
        "# delete unneeded folders\n",
        "!rm -rf \"/content/Indian Food Images/\"\n",
        "!rm -rf \"/content/our_data/Indian Food Images/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJAwRBEPDgIW"
      },
      "outputs": [],
      "source": [
        "!mv /content/Biryani-resize /content/our_data/\n",
        "!mv /content/Chole-Bhature-Resized /content/our_data/\n",
        "!mv /content/Jalebi-Resize /content/our_data/\n",
        "!mv /content/Kofta-Resized /content/our_data/\n",
        "!mv /content/Naan-Resized /content/our_data/\n",
        "!mv /content/Paneer-Tikka-resized /content/our_data/\n",
        "!mv /content/Pani-Puri-resize /content/our_data/\n",
        "!mv /content/Pav-Bhaji-Resized/ /content/our_data/\n",
        "!mv /content/Vadapav-Resized/ /content/our_data/\n",
        "!mv /content/dabeli-resize /content/our_data/\n",
        "!mv /content/dal_resized /content/our_data/\n",
        "!mv /content/dhokla-resize /content/our_data/\n",
        "!mv /content/dosa_resized /content/our_data/\n",
        "!mv /content/kathi-resize /content/our_data/\n",
        "!mv /content/pakora-resize /content/our_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ItwijB_PdagE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "from PIL import Image\n",
        "\n",
        "def moveFiles(source_folder, destination_folder):\n",
        "  # fetch all files\n",
        "  for file_name in os.listdir(source_folder):\n",
        "      file_name_jpg = file_name[:-4] + \".jpg\"\n",
        "\n",
        "      # construct full file path\n",
        "      source = source_folder + file_name\n",
        "\n",
        "      ### convert .png to .jpg\n",
        "      im1 = Image.open(r\"{}\".format(source))\n",
        "      # remove alpha channel (RGBA)\n",
        "      im1 = im1.convert('RGB')\n",
        "\n",
        "      jpg_filepath = source_folder + file_name_jpg\n",
        "      im1.save(r\"{}\".format(jpg_filepath))\n",
        "      ### \n",
        "\n",
        "      source_jpg = source_folder + file_name_jpg\n",
        "      destination = destination_folder + file_name_jpg\n",
        "\n",
        "      # move only .jpg files\n",
        "      if os.path.isfile(source_jpg):\n",
        "          shutil.move(source_jpg, destination)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xyDA0ZEcR47"
      },
      "outputs": [],
      "source": [
        "# make new folders for same resize-named folders \n",
        "!mkdir \"/content/our_data/chole_bhature\"\n",
        "!mkdir \"/content/our_data/paneer_tikka\"\n",
        "!mkdir \"/content/our_data/paneer_puri\"\n",
        "!mkdir \"/content/our_data/pav_bhaji\"\n",
        "!mkdir \"/content/our_data/vadapav\"\n",
        "!mkdir \"/content/our_data/dal\"\n",
        "!mkdir \"/content/our_data/pakora\"\n",
        "!mkdir \"/content/our_data/dhokla\"\n",
        "!mkdir \"/content/our_data/dosa\"\n",
        "!mkdir \"/content/our_data/dabeli\"\n",
        "!mkdir \"/content/our_data/kathi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ob8lK_PDeelX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58fb9689-61b8-4671-c46d-55fa13cc6869"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/PIL/Image.py:960: UserWarning: Palette images with Transparency expressed in bytes should be converted to RGBA images\n",
            "  \"Palette images with Transparency expressed in bytes should be \"\n"
          ]
        }
      ],
      "source": [
        "moveFiles(\"/content/our_data/Biryani-resize/resize/\",\"/content/our_data/biryani/\") \n",
        "moveFiles(\"/content/our_data/Chole-Bhature-Resized/Chole-Bhature-Resized/\",\"/content/our_data/chole_bhature/\")\n",
        "moveFiles(\"/content/our_data/Jalebi-Resize/Jalebi-Resize/\",\"/content/our_data/jalebi/\")\n",
        "moveFiles(\"/content/our_data/Kofta-Resized/Kofta-Resized/\",\"/content/our_data/kofta/\")\n",
        "moveFiles(\"/content/our_data/Naan-Resized/Naan-Resized/\",\"/content/our_data/naan/\")\n",
        "moveFiles(\"/content/our_data/Paneer-Tikka-resized/Paneer-Tikka-resized/\",\"/content/our_data/paneer_tikka/\")\n",
        "moveFiles(\"/content/our_data/Pani-Puri-resize/resize/\",\"/content/our_data/paneer_puri/\")\n",
        "moveFiles(\"/content/our_data/Pav-Bhaji-Resized/Pav-Bhaji-Resized/\",\"/content/our_data/pav_bhaji/\")\n",
        "moveFiles(\"/content/our_data/Vadapav-Resized/Vadapav-Resized/\",\"/content/our_data/vadapav/\")\n",
        "moveFiles(\"/content/our_data/dal_resized/dal_resized/\",\"/content/our_data/dal/\")\n",
        "moveFiles(\"/content/our_data/pakora-resize/pakora-resize/\",\"/content/our_data/pakora/\")\n",
        "moveFiles(\"/content/our_data/dhokla-resize/dhokla-resize/\",\"/content/our_data/dhokla/\")\n",
        "moveFiles(\"/content/our_data/dosa_resized/dosa_resized/\",\"/content/our_data/dosa/\")\n",
        "moveFiles(\"/content/our_data/dabeli-resize/dabeli-resize/\",\"/content/our_data/dabeli/\")\n",
        "moveFiles(\"/content/our_data/kathi-resize/kathi-resize/\",\"/content/our_data/kathi/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t9WU-XFwuybC"
      },
      "outputs": [],
      "source": [
        "# remove unnecessary folders after moving files\n",
        "!rm -rf \"/content/our_data/kathi-resize/\" \"/content/our_data/dabeli-resize\" \"/content/our_data/dosa_resized\" \"/content/our_data/dhokla-resize/\" \"/content/our_data/pakora-resize/\" \"/content/our_data/dal_resized/\" \"/content/our_data/Vadapav-Resized/\" \"/content/our_data/Biryani-resize/\" \"/content/our_data/Chole-Bhature-Resized/\" \"/content/our_data/Jalebi-Resize/\" \"/content/our_data/Kofta-Resized/\" \"/content/our_data/Naan-Resized/\" \"/content/our_data/Paneer-Tikka-resized/\" \"/content/our_data/Pani-Puri-resize/\" \"/content/our_data/Pav-Bhaji-Resized/\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtvWSi2KFjZ8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8066ab0-3eaf-4745-cba0-190972153e59"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting split-folders\n",
            "  Downloading split_folders-0.5.1-py3-none-any.whl (8.4 kB)\n",
            "Installing collected packages: split-folders\n",
            "Successfully installed split-folders-0.5.1\n"
          ]
        }
      ],
      "source": [
        "!pip install split-folders"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gNYkYuoXFlmo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7555df12-a99a-48ea-f0d7-59e5c524063a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Copying files: 8770 files [00:03, 2904.63 files/s]\n"
          ]
        }
      ],
      "source": [
        "# splitfolders will split folders into train, valid, and test folders\n",
        "import splitfolders\n",
        "splitfolders.ratio('/content/our_data/', output=\"/content/our_data\", seed=1337, ratio=(.4, 0.4,0.2)) \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vstFQ2G-nZxa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "46df6052-e1d1-4ccc-986c-0e5ff3dc0e9a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3502 images belonging to 91 classes.\n",
            "Found 3502 images belonging to 91 classes.\n"
          ]
        }
      ],
      "source": [
        "# preprocessing: data augmentation\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "datagen_train = ImageDataGenerator(\n",
        "    rotation_range=10,  \n",
        "    zoom_range=0.2, \n",
        "    width_shift_range=0.4,  \n",
        "    height_shift_range=0.1, \n",
        "    horizontal_flip=True, \n",
        "    vertical_flip=True,\n",
        "    rescale = 1./255,\n",
        ")  \n",
        "\n",
        "datagen_valid = ImageDataGenerator(rescale = 1./255,)\n",
        "\n",
        "# input matrices\n",
        "# load and iterate training dataset\n",
        "train_it = datagen_train.flow_from_directory(\n",
        "    \"/content/our_data/train/\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "# load and iterate validation dataset\n",
        "valid_it = datagen_valid.flow_from_directory(\n",
        "    \"/content/our_data/val/\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DHY0QcaUwKyP"
      },
      "outputs": [],
      "source": [
        "from keras.models import Sequential, Model\n",
        "from keras.layers import Dense, Flatten, Conv2D, MaxPool2D, GlobalAveragePooling2D, Dropout\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "from keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "nb_classes = 91\n",
        "model = Sequential()\n",
        "#model.add(Dense(64, activation='relu', kernel_initializer=HeNormal(), input_shape=(224, 224, 3)))\n",
        "#model.add(MaxPool2D())\n",
        "#model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer=HeNormal()))\n",
        "#model.add(Flatten())\n",
        "model.add(GlobalAveragePooling2D())\n",
        "model.add(Dense(256, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(Dense(128, activation='relu', kernel_initializer=HeNormal()))\n",
        "model.add(Dense(nb_classes, activation='softmax'))\n",
        "\n",
        "#callbacks\n",
        "es = EarlyStopping(patience=3, restore_best_weights=True)\n",
        "\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qphDK7__wjX9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "352bfb86-799c-4f73-92b7-5caeb3df260d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/35\n",
            "109/109 [==============================] - 76s 688ms/step - loss: 4.1515 - accuracy: 0.0677 - val_loss: 4.0452 - val_accuracy: 0.0834\n",
            "Epoch 2/35\n",
            "109/109 [==============================] - 75s 685ms/step - loss: 4.0492 - accuracy: 0.0897 - val_loss: 4.0144 - val_accuracy: 0.1002\n",
            "Epoch 3/35\n",
            "109/109 [==============================] - 73s 671ms/step - loss: 4.0188 - accuracy: 0.0979 - val_loss: 3.9915 - val_accuracy: 0.1059\n",
            "Epoch 4/35\n",
            "109/109 [==============================] - 73s 669ms/step - loss: 4.0053 - accuracy: 0.0951 - val_loss: 3.9810 - val_accuracy: 0.0962\n",
            "Epoch 5/35\n",
            "109/109 [==============================] - 73s 666ms/step - loss: 3.9827 - accuracy: 0.0931 - val_loss: 3.9750 - val_accuracy: 0.1028\n",
            "Epoch 6/35\n",
            "109/109 [==============================] - 71s 652ms/step - loss: 3.9754 - accuracy: 0.0957 - val_loss: 3.9779 - val_accuracy: 0.0965\n",
            "Epoch 7/35\n",
            "109/109 [==============================] - 70s 645ms/step - loss: 3.9605 - accuracy: 0.0914 - val_loss: 3.9571 - val_accuracy: 0.1017\n",
            "Epoch 8/35\n",
            "109/109 [==============================] - 71s 652ms/step - loss: 3.9425 - accuracy: 0.0908 - val_loss: 3.9464 - val_accuracy: 0.0997\n",
            "Epoch 9/35\n",
            "109/109 [==============================] - 70s 644ms/step - loss: 3.9354 - accuracy: 0.0971 - val_loss: 3.9300 - val_accuracy: 0.1051\n",
            "Epoch 10/35\n",
            "109/109 [==============================] - 70s 638ms/step - loss: 3.9190 - accuracy: 0.1011 - val_loss: 3.9161 - val_accuracy: 0.1088\n",
            "Epoch 11/35\n",
            "109/109 [==============================] - 71s 654ms/step - loss: 3.9000 - accuracy: 0.1011 - val_loss: 3.9074 - val_accuracy: 0.1011\n",
            "Epoch 12/35\n",
            "109/109 [==============================] - 76s 693ms/step - loss: 3.8948 - accuracy: 0.1031 - val_loss: 3.9011 - val_accuracy: 0.1042\n",
            "Epoch 13/35\n",
            "109/109 [==============================] - 72s 662ms/step - loss: 3.8817 - accuracy: 0.1011 - val_loss: 3.8938 - val_accuracy: 0.1042\n",
            "Epoch 14/35\n",
            "109/109 [==============================] - 75s 688ms/step - loss: 3.8708 - accuracy: 0.1062 - val_loss: 3.8805 - val_accuracy: 0.1097\n",
            "Epoch 15/35\n",
            "109/109 [==============================] - 74s 674ms/step - loss: 3.8708 - accuracy: 0.1008 - val_loss: 3.8847 - val_accuracy: 0.1119\n",
            "Epoch 16/35\n",
            "109/109 [==============================] - 73s 668ms/step - loss: 3.8579 - accuracy: 0.1074 - val_loss: 3.8731 - val_accuracy: 0.1142\n",
            "Epoch 17/35\n",
            "109/109 [==============================] - 72s 660ms/step - loss: 3.8532 - accuracy: 0.1037 - val_loss: 3.8707 - val_accuracy: 0.1079\n",
            "Epoch 18/35\n",
            "109/109 [==============================] - 72s 663ms/step - loss: 3.8438 - accuracy: 0.1005 - val_loss: 3.8659 - val_accuracy: 0.1114\n",
            "Epoch 19/35\n",
            "109/109 [==============================] - 73s 668ms/step - loss: 3.8454 - accuracy: 0.1059 - val_loss: 3.8822 - val_accuracy: 0.0974\n",
            "Epoch 20/35\n",
            "109/109 [==============================] - 77s 705ms/step - loss: 3.8358 - accuracy: 0.1017 - val_loss: 3.8893 - val_accuracy: 0.1094\n",
            "Epoch 21/35\n",
            "109/109 [==============================] - 79s 728ms/step - loss: 3.8394 - accuracy: 0.1034 - val_loss: 3.8613 - val_accuracy: 0.1159\n",
            "Epoch 22/35\n",
            "109/109 [==============================] - 74s 676ms/step - loss: 3.8314 - accuracy: 0.1077 - val_loss: 3.8607 - val_accuracy: 0.1176\n",
            "Epoch 23/35\n",
            "109/109 [==============================] - 72s 659ms/step - loss: 3.8266 - accuracy: 0.1085 - val_loss: 3.8668 - val_accuracy: 0.1148\n",
            "Epoch 24/35\n",
            "109/109 [==============================] - 71s 650ms/step - loss: 3.8231 - accuracy: 0.1071 - val_loss: 3.8714 - val_accuracy: 0.1068\n",
            "Epoch 25/35\n",
            "109/109 [==============================] - 75s 689ms/step - loss: 3.8139 - accuracy: 0.1059 - val_loss: 3.8599 - val_accuracy: 0.1062\n",
            "Epoch 26/35\n",
            "109/109 [==============================] - 99s 909ms/step - loss: 3.8170 - accuracy: 0.1048 - val_loss: 3.8563 - val_accuracy: 0.1168\n",
            "Epoch 27/35\n",
            "109/109 [==============================] - 74s 679ms/step - loss: 3.8154 - accuracy: 0.1062 - val_loss: 3.8752 - val_accuracy: 0.1074\n",
            "Epoch 28/35\n",
            "109/109 [==============================] - 74s 682ms/step - loss: 3.8071 - accuracy: 0.1108 - val_loss: 3.8648 - val_accuracy: 0.1165\n",
            "Epoch 29/35\n",
            "109/109 [==============================] - 76s 693ms/step - loss: 3.8086 - accuracy: 0.1085 - val_loss: 3.8644 - val_accuracy: 0.1122\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0a598ffc10>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "model.fit(train_it,\n",
        "          steps_per_epoch=3502/32,\n",
        "          validation_data=valid_it,\n",
        "          validation_steps=3502/32,\n",
        "          epochs=35,\n",
        "          callbacks=[es])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "datagen_test = ImageDataGenerator(rescale = 1./255,)\n",
        "test_it = datagen_test.flow_from_directory(\n",
        "    \"/content/our_data/test/\",\n",
        "    target_size=(224, 224),\n",
        "    batch_size=32,\n",
        "    color_mode=\"rgb\",\n",
        "    class_mode=\"categorical\",\n",
        ")"
      ],
      "metadata": {
        "id": "eXis88o9uuP5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0cd8f90a-fc69-4c2d-c9ee-91bef38cd070"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 1766 images belonging to 91 classes.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(model.evaluate(test_it))"
      ],
      "metadata": {
        "id": "TBnm02K2ldUX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "157b9196-d9f4-482c-9683-21f4646b4c47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56/56 [==============================] - 12s 206ms/step - loss: 3.9857 - accuracy: 0.0929\n",
            "[3.985685348510742, 0.0928652286529541]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8bzzPvz_QT4"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications.inception_v3 import InceptionV3\n",
        "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
        "import numpy as np\n",
        "from tensorflow.keras.initializers import HeNormal\n",
        "\n",
        "base_model =  InceptionV3(weights='imagenet', include_top=False)\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "x = Dense(256, activation='relu', kernel_initializer=HeNormal())(x)\n",
        "x = Dense(128, activation='relu', kernel_initializer=HeNormal())(x)\n",
        "predictions = Dense(91, activation='softmax')(x)\n",
        "inception_model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gCPt_59hBUs-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c4367159-67c0-4663-d9c7-7fda17281898"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/15\n",
            "109/109 [==============================] - 118s 1s/step - loss: 3.4496 - accuracy: 0.2344 - val_loss: 3.9591 - val_accuracy: 0.1476\n",
            "Epoch 2/15\n",
            "109/109 [==============================] - 103s 938ms/step - loss: 3.0466 - accuracy: 0.2955 - val_loss: 4.2701 - val_accuracy: 0.1750\n",
            "Epoch 3/15\n",
            "109/109 [==============================] - 104s 951ms/step - loss: 2.7912 - accuracy: 0.3358 - val_loss: 3.9878 - val_accuracy: 0.1539\n",
            "Epoch 4/15\n",
            "109/109 [==============================] - 106s 973ms/step - loss: 2.5900 - accuracy: 0.3721 - val_loss: 4.2248 - val_accuracy: 0.1839\n",
            "Epoch 5/15\n",
            "109/109 [==============================] - 104s 947ms/step - loss: 2.4168 - accuracy: 0.4009 - val_loss: 6.0311 - val_accuracy: 0.1690\n",
            "Epoch 6/15\n",
            "109/109 [==============================] - 102s 935ms/step - loss: 2.2905 - accuracy: 0.4306 - val_loss: 4.2329 - val_accuracy: 0.2167\n",
            "Epoch 7/15\n",
            "109/109 [==============================] - 105s 963ms/step - loss: 2.1614 - accuracy: 0.4457 - val_loss: 7.3189 - val_accuracy: 0.0925\n",
            "Epoch 8/15\n",
            "109/109 [==============================] - 103s 940ms/step - loss: 2.0852 - accuracy: 0.4600 - val_loss: 2.4614 - val_accuracy: 0.4326\n",
            "Epoch 9/15\n",
            "109/109 [==============================] - 104s 952ms/step - loss: 1.9402 - accuracy: 0.4860 - val_loss: 2.9727 - val_accuracy: 0.3015\n",
            "Epoch 10/15\n",
            "109/109 [==============================] - 104s 953ms/step - loss: 1.8877 - accuracy: 0.5040 - val_loss: 4.3316 - val_accuracy: 0.2710\n",
            "Epoch 11/15\n",
            "109/109 [==============================] - 104s 948ms/step - loss: 1.8027 - accuracy: 0.5297 - val_loss: 2.6805 - val_accuracy: 0.3926\n",
            "Epoch 12/15\n",
            "109/109 [==============================] - 103s 937ms/step - loss: 1.7518 - accuracy: 0.5260 - val_loss: 2.5664 - val_accuracy: 0.4506\n",
            "Epoch 13/15\n",
            "109/109 [==============================] - 105s 959ms/step - loss: 1.6358 - accuracy: 0.5465 - val_loss: 3.1290 - val_accuracy: 0.3378\n",
            "Epoch 14/15\n",
            "109/109 [==============================] - 106s 969ms/step - loss: 1.5722 - accuracy: 0.5608 - val_loss: 2.5726 - val_accuracy: 0.4089\n",
            "Epoch 15/15\n",
            "109/109 [==============================] - 106s 971ms/step - loss: 1.5184 - accuracy: 0.5851 - val_loss: 3.0382 - val_accuracy: 0.3935\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0b402673d0>"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "inception_model.compile(optimizer='adam', metrics=['accuracy'],\n",
        "              loss='categorical_crossentropy')\n",
        "\n",
        "inception_model.fit(train_it,\n",
        "          steps_per_epoch=3502/32, \n",
        "          validation_data=valid_it,\n",
        "          validation_steps=3502/32,\n",
        "          epochs=35)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(inception_model.evaluate(test_it))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bli2nOS57TTi",
        "outputId": "7aebc0d3-4c9a-49d6-8bf6-6e33248f8a5f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56/56 [==============================] - 14s 251ms/step - loss: 3.0386 - accuracy: 0.3851\n",
            "[3.038571357727051, 0.3850509524345398]\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "indianFoodClassification_pretrainAndRandomModels.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}